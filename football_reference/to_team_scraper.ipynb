{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Building out scraper for nfl team stats\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating a range of ints by 100 to append to url at the end\n",
    "# to navigate between pages\n",
    "leaf = range(0, 9100, 100)\n",
    "\n",
    "## root url below:\n",
    "base_url = ('http://www.pro-football-reference.com/play-index/tgl_finder.cgi?request=1&match=game&year_min=1999&year_max=2016&game_type=&game_num_min=0&game_num_max=99&week_num_min=0&week_num_max=99&game_day_of_week=&game_time=&time_zone=&surface=&roof=&temperature=&temperature_gtlt=lt&game_location=&game_result=&overtime=&league_id=NFL&team_id=&opp_id=&team_div_id=&opp_div_id=&team_conf_id=&opp_conf_id=&date_from=&date_to=&team_off_scheme=&opp_off_scheme=&team_def_align=&opp_def_align=&stadium_id=&c1stat=points&c1comp=gt&c1val=&c2stat=tot_yds&c2comp=gt&c2val=&c3stat=pass_cmp_opp&c3comp=gt&c3val=&c4stat=rush_att_opp&c4comp=gt&c4val=&c5comp=&c5gtlt=lt&c6mult=1.0&c6comp=&order_by=game_date&order_by_asc=&matching_games=1&conference_game=&division_game=&tm_is_playoff=&opp_is_playoff=&tm_is_winning=&opp_is_winning=&tm_scored_first=&tm_led=&tm_trailed=&tm_won_toss=&offset=')\n",
    "\n",
    "## this will loop through the leaf list and creat my entire list of urls\n",
    "## and append to the empty list \"url_list\"\n",
    "url_list = []\n",
    "for i in leaf:\n",
    "    url_list.append(base_url + str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## I am now creating a test link list so that I can work through the code\n",
    "## and debug while working with a less extensive dataset\n",
    "\n",
    "test_urls = url_list[0:1]\n",
    "\n",
    "## as you can see below, my test_urls list is composed of only first link\n",
    "print test_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = []\n",
    "\n",
    "html = urllib.urlopen(test_urls[0])\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "tab_header = soup.findAll('th')\n",
    "for i in tab_header:\n",
    "    headers.append(i.renderContents())\n",
    "print len(headers)\n",
    "\n",
    "## because of some extra headers mixed in, the actual column headers\n",
    "## that we want are interspersed in the following range\n",
    "print headers[6:42]\n",
    "\n",
    "## these names are a bit confusing due to the organization in the table\n",
    "## so they will probably just be manually recorded later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## The below code is an effort to scrape the stats for each game since 1999\n",
    "\n",
    "## I am manually recording a list of column names and changing them to\n",
    "## more easily interpretable names\n",
    "team_data_cols = ['rk', 'team', 'year', 'date', 'east_time', 'loc_time', 'blank_@',\n",
    "                 'opp', 'week', 'team_game#', 'day', 'result', 'ot', 'pf', 'pa', 'pdiff',\n",
    "                 'pcomb', 'tot_yds', 'off_plys', 'off_yds/ply', 'def_plys', 'def_yds/ply', 'to_lost',\n",
    "                  'off_time_poss','game_duration', 'opp_completions', 'opp_pass_att', 'opp_comp_perc',\n",
    "                 'opp_pass_yds', 'opp_pass_tds', 'opp_int_thrown', 'opp_sacks_taken', 'opp_sacks_yds_lost',\n",
    "                 'opp_rush_atts', 'opp_rush_yds', 'opp_rush_yds/att', 'opp_rush_td']\n",
    "\n",
    "## redefining our html and soup just to make sure I am referencing the\n",
    "## correct link and soup\n",
    "html = urllib.urlopen(test_urls[0])\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "## creating an empty list to append the data to\n",
    "data_points = []\n",
    "\n",
    "## creating variable that consists of the body of webpage we are interested\n",
    "## in combing through\n",
    "body = soup.findAll('tbody')\n",
    "\n",
    "## this variable creates a mass of the individual rows of the data to\n",
    "## work our way through\n",
    "indiv_rows = body[0].findAll('td')\n",
    "\n",
    "## i will loop through each row to strip out the data\n",
    "for row in indiv_rows:\n",
    "    # the line below redefines my soup as the contents of each individual row\n",
    "    inner_soup = BeautifulSoup(row.renderContents(), 'html.parser')\n",
    "    # this adds the data to the empty list\n",
    "    # the .text function strips hyperlinks and returns the text value only\n",
    "    data_points.append(inner_soup.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## since data points is just one long list, i need to break it up into\n",
    "## individual chunks so it can be added to our dataframe\n",
    "chunks = [data_points[x:x+37] for x in range(0, len(data_points), 37)]\n",
    "\n",
    "## Here I create a dataframe from the newly formed chunks and have\n",
    "## the newly defined columns as my column names\n",
    "team_df = pd.DataFrame(chunks, columns = team_data_cols)\n",
    "team_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### I am going to consolidate all the above code and unleash on all\n",
    "### of the webpages we are going for in order to scrape all the data\n",
    "\n",
    "## i am also importing time to put a sleep time in my loop\n",
    "import time\n",
    "\n",
    "leaf = range(0, 9100, 100)\n",
    "\n",
    "base_url = ('http://www.pro-football-reference.com/play-index/tgl_finder.cgi?request=1&match=game&year_min=1999&year_max=2016&game_type=&game_num_min=0&game_num_max=99&week_num_min=0&week_num_max=99&game_day_of_week=&game_time=&time_zone=&surface=&roof=&temperature=&temperature_gtlt=lt&game_location=&game_result=&overtime=&league_id=NFL&team_id=&opp_id=&team_div_id=&opp_div_id=&team_conf_id=&opp_conf_id=&date_from=&date_to=&team_off_scheme=&opp_off_scheme=&team_def_align=&opp_def_align=&stadium_id=&c1stat=points&c1comp=gt&c1val=&c2stat=tot_yds&c2comp=gt&c2val=&c3stat=pass_cmp_opp&c3comp=gt&c3val=&c4stat=rush_att_opp&c4comp=gt&c4val=&c5comp=&c5gtlt=lt&c6mult=1.0&c6comp=&order_by=game_date&order_by_asc=&matching_games=1&conference_game=&division_game=&tm_is_playoff=&opp_is_playoff=&tm_is_winning=&opp_is_winning=&tm_scored_first=&tm_led=&tm_trailed=&tm_won_toss=&offset=')\n",
    "\n",
    "url_list = []\n",
    "for i in leaf:\n",
    "    url_list.append(base_url + str(i))\n",
    "    \n",
    "team_data_cols = ['rk', 'team', 'year', 'date', 'east_time', 'loc_time', 'blank_@',\n",
    "                 'opp', 'week', 'team_game#', 'day', 'result', 'ot', 'pf', 'pa', 'pdiff',\n",
    "                 'pcomb', 'tot_yds', 'off_plys', 'off_yds/ply', 'def_plys', 'def_yds/ply', 'to_lost',\n",
    "                  'off_time_poss','game_duration', 'opp_completions', 'opp_pass_att', 'opp_comp_perc',\n",
    "                 'opp_pass_yds', 'opp_pass_tds', 'opp_int_thrown', 'opp_sacks_taken', 'opp_sacks_yds_lost',\n",
    "                 'opp_rush_atts', 'opp_rush_yds', 'opp_rush_yds/att', 'opp_rush_td']\n",
    "\n",
    "data_points = []\n",
    "\n",
    "## this initial loop has been added to circulate through all the links collected\n",
    "# also adding count to check my status while conducting the loop\n",
    "count = 0\n",
    "for i in url_list:\n",
    "    html = urllib.urlopen(i)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    body = soup.findAll('tbody')\n",
    "    indiv_rows = body[0].findAll('td')\n",
    "    for row in indiv_rows:\n",
    "        inner_soup = BeautifulSoup(row.renderContents(), 'html.parser')\n",
    "        data_points.append(inner_soup.text)\n",
    "    ## adding a sleep time to not ping website too frequently\n",
    "    count += 1\n",
    "    print \"you have completed this many loops:  %d\" % count\n",
    "    #time.sleep(1)\n",
    "\n",
    "chunks = [data_points[x:x+37] for x in range(0, len(data_points), 37)]\n",
    "\n",
    "team_df = pd.DataFrame(chunks, columns = team_data_cols)\n",
    "team_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__I now have a dataframe that I will begin cleaning and formatting so that I can export a workable dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print team_df.columns\n",
    "team_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this will drop the first col which is a repeat of the index\n",
    "team_df = team_df.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "## this code will mark every game with @ in the column as an away game\n",
    "team_df['blank_@'] = team_df['blank_@'].map({'@':'away'})\n",
    "\n",
    "## this will mark all the na's as home games\n",
    "team_df['blank_@'].fillna(value = 'home', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this renames the column as the game location\n",
    "team_df.rename(columns = {'blank_@':'location'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "team_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## checking to see where null values are to fix\n",
    "team_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print team_df.ot.value_counts()\n",
    "print\n",
    "## i am going to change these to zero for no ot and 1 for ot\n",
    "team_df['ot'] = team_df['ot'].fillna(0)\n",
    "\n",
    "## checking to confirm it worked correctly\n",
    "print team_df.ot.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## replacding the OT values with a 1\n",
    "team_df.ot.replace('OT', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "team_df.ot.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## finding the remainder of the null values\n",
    "team_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## checking value counts to confirm my thought that the nulls are zeros here\n",
    "print team_df.to_lost.value_counts()\n",
    "\n",
    "## going to fill na's with the value zero\n",
    "team_df.to_lost.fillna(0, inplace = True)\n",
    "\n",
    "## checking to confirm that all 1851 nulls are now zeros\n",
    "print team_df.to_lost.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## checking datatypes to see if there is anything that currently needs to be changed\n",
    "team_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## I want to create a win/loss column by itself\n",
    "## creating an empty column to append data to\n",
    "#team_df['win_loss'] = team_df.result\n",
    "\n",
    "# creating an empty list and appending the first character of each row to it\n",
    "wl_list = []\n",
    "wl_list\n",
    "for i in team_df.result:\n",
    "    wl_list.append(i[0])\n",
    "wl_list = pd.Series(wl_list)\n",
    "\n",
    "\n",
    "## I do the first character only because it is either a L, W, or T for tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this creates a new column (win_loss) and sets it equal to the wl_list series\n",
    "team_df['win_loss'] = wl_list\n",
    "team_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this is a check to confirm that the correct changes were made\n",
    "team_df.win_loss.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## this shows the rows where the game ended in a tie\n",
    "team_df[team_df.win_loss.str.contains('T')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print team_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## I am creating a csv file from the newly formed team_df and\n",
    "## exporting to to my current working directory\n",
    "team_df.to_csv('team_data_df', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## I will now be creating a database in postgres in order to add\n",
    "## this dataframe as a table to perform queries on outside of python\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "engine = create_engine('postgresql://TerryONeill@localhost:5432/nfl_capstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this is adding the dataframe to my newly created database in psql as\n",
    "## a table named 'team_data_table'\n",
    "team_df.to_sql('team_data_table', engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
